{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yRxF3fGk83TKVLrMatsfF6",
     "type": "MD"
    }
   },
   "source": [
    "# <u>Submission instructions</u>\n",
    "### Submission must be in pairs, unless otherwise authorized.\n",
    "#### Submit by 28/2/2024\n",
    "\n",
    "<ul style=\"font-size: 17px\">\n",
    "<li> This notebook contains all the questions. You should follow the instructions below.</li>\n",
    "<li> Solutions for both theoretical and practical parts should be written in this notebook</li>\n",
    "</ul>\n",
    "\n",
    "<h3> Moodle submission</h3>\n",
    "\n",
    "\n",
    "<p style=\"font-size: 17px\">\n",
    "You should submit three files:\n",
    "</p>\n",
    "<ul style=\"font-size: 17px\">\n",
    "<li>IPYNB notebook:\n",
    "  <ul>\n",
    "  <li>All the wet and dry parts, including code, graphs, discussion, etc.</li>\n",
    "  </ul>\n",
    "</li>\n",
    "<li>PDF file:\n",
    "  <ul>\n",
    "  <li>Export the notebook to PDF. Make sure that all the cells are visible.</li>\n",
    "  </ul>\n",
    "</li>\n",
    "<li>Pickle file:\n",
    "  <ul>\n",
    "    <li>As requested in Q2.a</li>\n",
    "  </ul>\n",
    "</li>\n",
    "</ul>\n",
    "<p style=\"font-size: 17px\">\n",
    "All files should be in the following format: \"HW1_ID1_ID2.file\"\n",
    "<br>\n",
    "Good Luck!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "mYiXOlyvoIiYs0VuDro4xC",
     "type": "MD"
    }
   },
   "source": [
    "<h1> Question 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "soZCVUP0PzcQexv4G9bDRg",
     "type": "MD"
    }
   },
   "source": [
    "## I. Softmax Derivative (10pt)\n",
    "\n",
    "<p style=\"font-size: 17px\">\n",
    "Derive the gradients of the softmax function and demonstrate how the expression can be reformulated solely by using the softmax function, i.e., in some expression where only $softmax(x)$, but not $x$, is present).\n",
    "Recall that the softmax function is defined as follows:\n",
    "$$softmax(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$softmax(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IpqCRRTyHjo6KCJA6LIqmx",
     "type": "MD"
    }
   },
   "source": [
    "### I. Softmax Derivative - Answer:\n",
    "$$\\frac{\\partial softmax(x)_i}{\\partial x_k} = \\frac{\\partial \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}}{\\partial x_k}$$\n",
    "$$when\\ i = j$$\n",
    "$$\\frac{\\partial softmax(x)_i}{\\partial x_k} = softmax(x)_i \\cdot (1 - softmax(x)_i)$$\n",
    "$$when\\ i \\neq j$$\n",
    "$$\\frac{\\partial softmax(x)_i}{\\partial x_k} = -softmax(x)_i \\cdot softmax(x)_k$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qm487O2HSrGtnaEuKf0Yet",
     "type": "MD"
    }
   },
   "source": [
    "## II. Cross-Entropy Gradient (10pt)\n",
    "<p style=\"font-size: 17px\">\n",
    "Derive the gradient of cross-entropy loss with regard to the inputs of a softmax function. i.e., find the gradients with respect to the softmax input vector $\\theta$, when the prediction is denoted by $\\hat{y} = softmax(\\theta)$. \n",
    "\n",
    "\n",
    "<p style=\"font-size: 17px\">where $y$ is the one-hot label vector, and $\\hat{y}$ is the predicted probability vector for all classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = softmax(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the cross entropy function is:\n",
    "$$CE(y, \\hat{y}) = -\\sum_i y_i log(\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "k0wyAAwjcE9zeKFTRfYW0U",
     "type": "MD"
    }
   },
   "source": [
    "### II. Cross-Entropy Gradient - Answer\n",
    "\n",
    "<!--- write your answer -->\n",
    "$$\\frac{\\partial CE(y, \\hat{y})}{\\partial\\theta} = \\frac{\\partial CE(y, \\hat{y})}{\\partial\\hat{y}}\\frac{\\partial\\hat{y}}{\\partial\\theta} = \\frac{\\partial -\\sum_i y_i log(\\hat{y_i})}{\\partial\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial CE}{\\partial \\theta_k} &= \\frac{\\partial}{\\partial \\theta_k} \\sum_{j=1}^n (-y_j \\log(\\sigma(\\theta_j))) \\\\\n",
    "&= - \\sum_{j=1}^n y_j \\frac{\\partial}{\\partial \\theta_k} \\log(\\sigma(\\theta_j)) & &\\text{...addition rule, } -y_j \\text{ is constant}\\\\\n",
    "&= - \\sum_{j=1}^n y_j \\frac{1}{\\sigma(\\theta_j)}  \\frac{\\partial}{\\partial \\theta_k}\\sigma(\\theta_j) & &\\text{...chain rule}\\\\\n",
    "&= -y_k \\frac{\\sigma(\\theta_k)(1-\\sigma(\\theta_k))}{\\sigma(\\theta_k)} + \\sum_{j\\neq k} y_j \\frac{\\sigma(\\theta_k)\\sigma(\\theta_j)}{\\sigma(\\theta_j)} & &\\text{...consider both } j=k \\text{ and } j\\neq k \\\\\n",
    "&= -y_k (1-\\sigma(\\theta_k)) + \\sum_{j\\neq k} y_j \\sigma(\\theta_k) \\\\\n",
    "&= -y_k + y_k\\sigma(\\theta_k) + \\sum_{j\\neq k} y_j \\sigma(\\theta_k) \\\\\n",
    "&= -y_k + \\sigma(\\theta_k) \\sum_j y_j. \\\\\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\Rightarrow \\frac{\\partial CE}{\\partial \\theta_k} &= \\sigma(\\theta_k) - y_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "XXTewYlH5FSAKliEp1LYVr",
     "type": "MD"
    }
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "eU1spAtEoWOmiuz8b1epc6",
     "type": "MD"
    }
   },
   "source": [
    "## I. Derivative Of Activation Functions (10pt)\n",
    "\n",
    "<p style=\"font-size: 17px\">\n",
    "The following cell contains an implementation of some activation functions. Implement the corresponding derivatives.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "KnegWR2iltc4XwyJw0Fdw3",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return torch.div(torch.exp(x) - torch.exp(-x), torch.exp(x) + torch.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = torch.exp(x.T - torch.max(x, dim=-1).values).T  # Subtracting max(x) for numerical stability\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "maCkjVZ0dIFKS1IPadow46",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 - (tanh(x)**2)\n",
    "\n",
    "\n",
    "def d_softmax(x):\n",
    "    s = softmax(x)\n",
    "    batch_size, n_classes = s.shape\n",
    "    # Initialize the Jacobian matrix with zeros\n",
    "    jacobian = torch.zeros((batch_size, n_classes))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(batch_size):\n",
    "                if j == i:\n",
    "                    jacobian[i, j, k] = s[i, j] * (1 - s[i, j])\n",
    "                else:\n",
    "                    jacobian[i, j, k] = -s[i, j] * s[i, k]\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8434, -2.1867],\n",
      "        [ 1.5041,  0.5034],\n",
      "        [ 0.3367, -1.1663]])\n",
      "Jacobian matrix of the softmax: tensor([[[ 0.0440, -0.0440],\n",
      "         [-0.0440,  0.0440]],\n",
      "\n",
      "        [[ 0.1965, -0.1965],\n",
      "         [-0.1965,  0.1965]],\n",
      "\n",
      "        [[ 0.1489, -0.1489],\n",
      "         [-0.1489,  0.1489]]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "x = torch.randn(3, 2) \n",
    "print(x) # Example input vector\n",
    "jacobian_matrix = d_softmax(x)\n",
    "print(\"Jacobian matrix of the softmax:\", jacobian_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GaitupBIkrSaDCvQc5buP5",
     "type": "MD"
    }
   },
   "source": [
    "## II. Train a Fully Connected network on MNIST (30pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "An2t0OVJDND2S2KElPOtHf",
     "type": "MD"
    }
   },
   "source": [
    "<p style=\"font-size: 17px\">In the following exercise, you will create a classifier for the MNIST dataset.\n",
    "You should write your own training and evaluation code and meet the following\n",
    "constraints:\n",
    "<ul>\n",
    "<li> You are only allowed to use torch tensor manipulations.</li>\n",
    "<li> You are NOT allowed to use:\n",
    "  <ul>\n",
    "  <li> Auto-differentiation - backward()</li>\n",
    "  <li> Built-in loss functions</li>\n",
    "  <li> Built-in activations</li>\n",
    "  <li> Built-in optimization</li>\n",
    "  <li> Built-in layers (torch.nn)</li>\n",
    "  </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RJnSAOesGYsgF2m9BMUfzK",
     "type": "MD"
    }
   },
   "source": [
    "<p style=\"font-size: 17px\">\n",
    " a) The required classifier class is defined.\n",
    "<ul style=\"font-size: 17px\">\n",
    "<li> You should implement the backward pass of the model.\n",
    "<li> Train the model and plot the model's accuracy and loss (both on train and test sets) as a function of the epochs.\n",
    "<li> You should save the model's weights and biases. Change the student_ids to yours.\n",
    "</ul>\n",
    "<p style=\"font-size: 17px\">In this section, you <b>must</b> use the \"set_seed\" function with the given seed and <b>sigmoid</b> as an activation function.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "aJySKnlSpLwCl0eoJYCaFN",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Constants\n",
    "SEED = 42\n",
    "EPOCHS = 16\n",
    "BATCH_SIZE = 32\n",
    "NUM_OF_CLASSES = 10\n",
    "\n",
    "# Setting seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# Transformation for the data\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torch.flatten])\n",
    "\n",
    "# Cross-Entropy loss implementation\n",
    "def one_hot(y, num_of_classes=10):\n",
    "    hot = torch.zeros((y.size()[0], num_of_classes))\n",
    "    hot[torch.arange(y.size()[0]), y] = 1\n",
    "    return hot\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    return -torch.sum(one_hot(y) * torch.log(y_hat)) / y.size()[0]\n",
    "\n",
    "def cross_entropy_builtin(y, y_hat_logits):\n",
    "    return F.cross_entropy(y_hat_logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "lo57lFKyOqQmMGfLR0Meso",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nuoAPQXRzhYGwelAr4CeWe",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class FullyConnectedNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_size1, activation_func = sigmoid, lr=0.01):\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "\n",
    "        # activation function\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.input_size, self.hidden_size1)\n",
    "        self.b1 = torch.zeros(self.hidden_size1)\n",
    "\n",
    "        self.W2 = torch.randn(self.hidden_size1, self.output_size)\n",
    "        self.b2 = torch.zeros(self.output_size)\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = torch.matmul(x, self.W1) + self.b1\n",
    "        self.h1 = self.activation_func(self.z1)\n",
    "        self.z2 = torch.matmul(self.h1, self.W2) + self.b2\n",
    "        self.y_hat = softmax(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, x, y, y_hat):\n",
    "        lr = self.lr\n",
    "        batch_size = y.size(0)\n",
    "        \n",
    "        dl_dy_hat = (1/batch_size)*((y_hat - y)/ (y_hat * (torch.ones(y_hat.shape[-1]) - y_hat))) \n",
    "        print(dl_dy_hat.shape)\n",
    "        print(d_softmax(self.z2).shape)\n",
    "        dl_dz2 =  dl_dy_hat * d_softmax(self.z2) \n",
    "        \n",
    "        dl_dW2 = torch.matmul(torch.t(self.h), dl_dz2)\n",
    "        dl_db2 = torch.matmul(torch.t(dl_dz2), torch.ones(batch_size))\n",
    "        \n",
    "        dl_dh = torch.matmul(dl_dz2, torch.t(self.W2)) \n",
    "        dl_dz1 = dl_dh * d_sigmoid(self.z1) \n",
    "        \n",
    "        dl_dW1 = torch.matmul(torch.t(x), dl_dz1) \n",
    "        dl_db1 = torch.matmul(torch.t(dl_dz1), torch.ones(batch_size))\n",
    "       \n",
    "\n",
    "        #gradient step\n",
    "        self.W1 -= lr*dl_dW1 \n",
    "        self.b1 -= lr*dl_db1\n",
    "        self.W2 -= lr*dl_dW2\n",
    "        self.b2 -= lr*dl_db2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for trainig a model\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "9KzKsJARUjbine4dVjkfg2",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "model = FullyConnectedNetwork(784, 10, 128, sigmoid, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (15 to go)\n",
      "torch.Size([32])\n",
      "torch.Size([32, 10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24072\\2607264493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch: {epoch+1} ({EPOCHS - (epoch+1)} to go)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mcalculate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mcalculate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24072\\2607264493.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[1;34m(dataloader, mode)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Backpropagation for training mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Calculate and store epoch metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24072\\865268754.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, x, y, y_hat)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_dy_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mdl_dz2\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdl_dy_hat\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mdl_dW2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_dz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Initialize history lists for tracking progress\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': []\n",
    "}\n",
    "\n",
    "# Function to calculate metrics for a given dataloader\n",
    "def calculate_metrics(dataloader, mode='train'):\n",
    "    total_loss, total_accuracy, total_samples = 0, 0, 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        y_hat = model.forward(x=X_batch)\n",
    "        loss = cross_entropy(y=y_batch, y_hat=y_hat)\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = (predicted == y_batch).float().mean()\n",
    "\n",
    "        \n",
    "        # Accumulate batch results\n",
    "        total_loss += loss * len(y_batch)\n",
    "        total_accuracy += accuracy\n",
    "        total_samples += len(y_batch)\n",
    "        \n",
    "        # Backpropagation for training mode\n",
    "        if mode == 'train':\n",
    "            model.backward(x=X_batch, y=y_batch, y_hat=y_hat.max(dim=1).values)\n",
    "    \n",
    "    # Calculate and store epoch metrics\n",
    "    history[f'{mode}_loss'].append(total_loss / total_samples)\n",
    "    history[f'{mode}_accuracy'].append(total_accuracy / total_samples)\n",
    "\n",
    "# Function to plot the training and testing loss and accuracy\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training and testing loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['test_loss'], label='Test Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot training and testing accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history['test_accuracy'], label='Test Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training and testing the model\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch: {epoch+1} ({EPOCHS - (epoch+1)} to go)')\n",
    "    \n",
    "    calculate_metrics(train_dataloader, 'train')\n",
    "    calculate_metrics(test_dataloader, 'test')\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "oNEyQXyvcWyjxSQwcivnEJ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "students_ids = \"12345789_987654321\"\n",
    "torch.save({\"W1\": model.W1, \"W2\": model.W2, \"b1\": model.b1, \"b2\": model.b2}, f\"HW1_{students_ids}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "bLjHSOjQThjOLvJ8hOgvr6",
     "type": "MD"
    }
   },
   "source": [
    "<p style=\"font-size: 17px\"> b) Train the model with various learning rates (at least 3).\n",
    "<ul style=\"font-size: 17px\">\n",
    "<li> Plot the model's accuracy and loss (both on train and test sets) as a function of the epochs.\n",
    "<li>Discuss the differences in training with different learning rates. Support your answer with plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "dn5x92wZhavymdcmZMKJ7u",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "8qd1yAbzAT1xIRQtWUUHoY",
     "type": "MD"
    }
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "WC7InslhxyIvCc38ESrBDr",
     "type": "MD"
    }
   },
   "source": [
    "## I. Implement and Train a CNN (30pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Tcu4zWCAjQRYmJRiqOBkVC",
     "type": "MD"
    }
   },
   "source": [
    "<p style=\"font-size: 17px\"> You are a data scientist at a supermarket. Your manager asked you to write a new image classifiaction algorithem for the self checkout cashiers. The images are of products from your grocery store (dataset files are attched in the Moodle).\n",
    "<br>\n",
    "Your code and meet the following constraints:\n",
    "<ul style=\"font-size: 17px\">\n",
    "<li> Your classifier must be CNN based</li>\n",
    "<li> You are not allowed to use any pre-trained model</li>\n",
    "</ul>\n",
    "<br>\n",
    "<p style=\"font-size: 17px\">\n",
    "In order to satisfy your boss you have to reach 65% accuracy on the test set. You will get a bonus for your salary (and 10 points to your grade) if your model's number of paramters is less than 100K. You can reutilize code from the tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ITjBAln5fGCruMZYHgjF35",
     "type": "MD"
    }
   },
   "source": [
    "<ul style=\"font-size: 17px\">\n",
    "<li>Train the model and plot the model's accuracy and loss (both on train and validation sets) as a function of the epochs. </li>\n",
    "<li>Report the test set accurecy.</li>\n",
    "<li>Discus the progress you made and describe your final model.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vSn3WoVprpB4oQzf2wu5sH",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ZcQZvSNVXmDSbthGEL39Ti",
     "type": "MD"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Fq3Hl4TmKAzC5Xw1b7iBpZ",
     "type": "MD"
    }
   },
   "source": [
    "## II. Analyzing a Pre-trained CNN (Filters) (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "1KKmAN8YX0fDzOJct7vTn4",
     "type": "MD"
    }
   },
   "source": [
    "In this part, you are going to analyze a (large) pre-trained model. Pre-trained models are quite popular these days, as big companies can train really large models on large datasets (something that personal users can't do as they lack the sufficient hardware). These pre-trained models can be used to fine-tune on other/small datasets or used as components in other tasks (like using a pre-trained classifier for object detection).\n",
    "\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
    "\n",
    "You can use the following transform to normalize:\n",
    "\n",
    "<code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</code>\n",
    "<a href=\"https://pytorch.org/vision/stable/models.html\">Read more here</a>\n",
    "\n",
    "\n",
    "1. Load a pre-trained VGG16 with PyTorch using torchvision.models.vgg16(pretrained=True, progress=True, **kwargs) (<a href=\"https://pytorch.org/vision/stable/models.html#classification\">read more here</a>). Don't forget to use the model in evaluation mode (<code>model.eval()</code>).\n",
    "\n",
    "2. Load the images in the 'birds' folder and display them.\n",
    "\n",
    "3. Pre-process the images to fit VGG16's architecture. What steps did you take?\n",
    "\n",
    "4. Feed the images (forward pass) to the model. What are the outputs?\n",
    "\n",
    "5. Choose an image of a dog in the 'dogs' folder, display it and feed it to network. What are the outputs?\n",
    "\n",
    "6. For the first 3 filters in the first layer of VGG16, plot the filters, and then plot their response (their output) for the image from question 5. Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0BzbPyE0k11Lv0oAVY4as0",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
